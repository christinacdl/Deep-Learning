{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_Christodoulou.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUEw4Qeq8wuG"
      },
      "source": [
        "Lesson M909\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bwkf10RvvrP"
      },
      "source": [
        "# Import necessary libraries\n",
        "import re  \n",
        "import pandas as pd  \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnkIw-Dfv1cw",
        "outputId": "94792d92-c466-4262-f8f3-6c8f24cd6561"
      },
      "source": [
        "# Download necessary packages\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r59nOihowAS4"
      },
      "source": [
        "stop_words_set = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iztOD6_00Xp",
        "outputId": "9e47c651-5feb-4628-b935-47435e51253a"
      },
      "source": [
        "# The dataset was downloaded from Kaggle and can be found here: https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews\n",
        "# It contains 20k reviews crawled from Tripadvisor.\n",
        "\n",
        "df = pd.read_csv('tripadvisor_hotel_reviews.csv')\n",
        "print('Shape of initial dataframe:',df.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of initial dataframe: (20491, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "3XrQCjhm1mLR",
        "outputId": "59201aaa-2c70-40a7-8013-162e3a29b3a0"
      },
      "source": [
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nice hotel expensive parking got good deal sta...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ok nothing special charge diamond member hilto...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great stay great stay, went seahawk game aweso...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20486</th>\n",
              "      <td>best kept secret 3rd time staying charm, not 5...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20487</th>\n",
              "      <td>great location price view hotel great quick pl...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20488</th>\n",
              "      <td>ok just looks nice modern outside, desk staff ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20489</th>\n",
              "      <td>hotel theft ruined vacation hotel opened sept ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20490</th>\n",
              "      <td>people talking, ca n't believe excellent ratin...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20491 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Review  Rating\n",
              "0      nice hotel expensive parking got good deal sta...       4\n",
              "1      ok nothing special charge diamond member hilto...       2\n",
              "2      nice rooms not 4* experience hotel monaco seat...       3\n",
              "3      unique, great stay, wonderful time hotel monac...       5\n",
              "4      great stay great stay, went seahawk game aweso...       5\n",
              "...                                                  ...     ...\n",
              "20486  best kept secret 3rd time staying charm, not 5...       5\n",
              "20487  great location price view hotel great quick pl...       4\n",
              "20488  ok just looks nice modern outside, desk staff ...       2\n",
              "20489  hotel theft ruined vacation hotel opened sept ...       1\n",
              "20490  people talking, ca n't believe excellent ratin...       2\n",
              "\n",
              "[20491 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb8HFqog9S45",
        "outputId": "fd873c9f-b98d-4ba2-9dec-222fe4adda1e"
      },
      "source": [
        "keys = df.keys() \n",
        "for key in keys:\n",
        "    df_len = len(df[key].unique()) # the length of the unique values of each column\n",
        "    print('{0:25}{1:10}'.format(key,df_len))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review                        20491\n",
            "Rating                            5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy15JC0W6cLx"
      },
      "source": [
        "**DATA PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skX5A7l_-XU4"
      },
      "source": [
        "def data_cleaning(dataframe):\n",
        "  \n",
        "  print('\\nData shape before removing missing values: ', dataframe.shape)\n",
        "  print('\\nWith missing values:')\n",
        "  print(dataframe.isna().sum()) # check for missing values\n",
        "\n",
        "  dataframe.dropna(inplace=True) # remove missing values and keep the dataFrame with valid entries in the same variable\n",
        "  dataframe.reset_index(inplace=True,drop=True)\n",
        "\n",
        "  print('\\nWithout missing values:')\n",
        "  print(dataframe.isna().sum())\n",
        "  print('\\nData shape after removing missing values: ', dataframe.shape)\n",
        "\n",
        "  dataframe.duplicated() # check for duplicates\n",
        "  return print('Number of duplicates in the dataframe:', dataframe.duplicated().sum())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGQ1Jo05_Jha"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "                \n",
        "        # Remove tags\n",
        "        TAG_RE = re.compile(r'<[^>]+>')\n",
        "        no_tags = TAG_RE.sub('',text)  \n",
        "                \n",
        "        # Remove unusual characters\n",
        "        text = re.sub('<[^>]*>', '', no_tags)\n",
        "    \n",
        "        # Remove emoticons\n",
        "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "                \n",
        "        # Convert all words to lowercase\n",
        "        text = re.sub('[\\W]+', ' ', text.lower()) + \" \".join(emoticons).replace('-', '')\n",
        "    \n",
        "        # Remove numbers\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Create tokens\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Remove punctuation\n",
        "        no_punct = [word for word in tokens if word.isalpha()]\n",
        "        \n",
        "        # Remove stopwords\n",
        "        words = [w for w in no_punct if not w in stop_words_set]\n",
        "             \n",
        "        return words"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIJPEwhR_CA_",
        "outputId": "eab5d353-52ed-4755-d9ec-e89e9b9026b1"
      },
      "source": [
        "data_cleaning(df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data shape before removing missing values:  (20491, 2)\n",
            "\n",
            "With missing values:\n",
            "Review    0\n",
            "Rating    0\n",
            "dtype: int64\n",
            "\n",
            "Without missing values:\n",
            "Review    0\n",
            "Rating    0\n",
            "dtype: int64\n",
            "\n",
            "Data shape after removing missing values:  (20491, 2)\n",
            "Number of duplicates in the dataframe: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iVJ90caP0JZ",
        "outputId": "b7bb25a0-3334-4654-af55-f7b2102d8b5c"
      },
      "source": [
        "# Apply the function to preprocess the texts in the reviews\n",
        "df['Review'] = df['Review'].apply(text_preprocessing)\n",
        "\n",
        "# Convert the dataframe into a list containing lists\n",
        "corpus = df['Review'].tolist()\n",
        "print('Length of corpus: ', len(corpus))\n",
        "print(corpus[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of corpus:  20491\n",
            "['ok', 'nothing', 'special', 'charge', 'diamond', 'member', 'hilton', 'decided', 'chain', 'shot', 'th', 'anniversary', 'seattle', 'start', 'booked', 'suite', 'paid', 'extra', 'website', 'description', 'suite', 'bedroom', 'bathroom', 'standard', 'hotel', 'room', 'took', 'printed', 'reservation', 'desk', 'showed', 'said', 'things', 'like', 'tv', 'couch', 'ect', 'desk', 'clerk', 'told', 'oh', 'mixed', 'suites', 'description', 'kimpton', 'website', 'sorry', 'free', 'breakfast', 'got', 'kidding', 'embassy', 'suits', 'sitting', 'room', 'bathroom', 'bedroom', 'unlike', 'kimpton', 'calls', 'suite', 'day', 'stay', 'offer', 'correct', 'false', 'advertising', 'send', 'kimpton', 'preferred', 'guest', 'website', 'email', 'asking', 'failure', 'provide', 'suite', 'advertised', 'website', 'reservation', 'description', 'furnished', 'hard', 'copy', 'reservation', 'printout', 'website', 'desk', 'manager', 'duty', 'reply', 'solution', 'send', 'email', 'trip', 'guest', 'survey', 'follow', 'email', 'mail', 'guess', 'tell', 'concerned', 'guest', 'staff', 'ranged', 'indifferent', 'helpful', 'asked', 'desk', 'good', 'breakfast', 'spots', 'neighborhood', 'hood', 'told', 'hotels', 'gee', 'best', 'breakfast', 'spots', 'seattle', 'block', 'away', 'convenient', 'hotel', 'know', 'exist', 'arrived', 'late', 'night', 'pm', 'inside', 'run', 'bellman', 'busy', 'chating', 'cell', 'phone', 'help', 'bags', 'prior', 'arrival', 'emailed', 'hotel', 'inform', 'th', 'anniversary', 'half', 'really', 'picky', 'wanted', 'make', 'sure', 'good', 'got', 'nice', 'email', 'saying', 'like', 'deliver', 'bottle', 'champagne', 'chocolate', 'covered', 'strawberries', 'room', 'arrival', 'celebrate', 'told', 'needed', 'foam', 'pillows', 'arrival', 'champagne', 'strawberries', 'foam', 'pillows', 'great', 'room', 'view', 'alley', 'high', 'rise', 'building', 'good', 'better', 'housekeeping', 'staff', 'cleaner', 'room', 'property', 'impressed', 'left', 'morning', 'shopping', 'room', 'got', 'short', 'trips', 'hours', 'beds', 'comfortable', 'good', 'ac', 'heat', 'control', 'x', 'inch', 'screen', 'bring', 'green', 'shine', 'directly', 'eyes', 'night', 'light', 'sensitive', 'tape', 'controls', 'start', 'hotel', 'clean', 'business', 'hotel', 'super', 'high', 'rates', 'better', 'chain', 'hotels', 'seattle']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUkS1clU1HXQ"
      },
      "source": [
        "**WORD2VEC MODEL 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnYMzwVbq6YY"
      },
      "source": [
        "**Set model parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvRd3cCxRANg"
      },
      "source": [
        "model1 = gensim.models.Word2Vec(size= 100, window = 3, min_count = 2, sg = 1, workers = 10, hs = 0, negative = 5)\n",
        "\n",
        "# Size of dimensionality of word vectors: 100\n",
        "# Window, the maximum distance between the target word and its neighboring word: 3\n",
        "# Minimum count, ignoring all words with total frequency lower than this, words must appear this many times to be in vocab: 2\n",
        "# Sg, training algorithm: Skip-Gram\n",
        "# The number of worker threads used to train the model: 10\n",
        "# 0: Negative sampling\n",
        "# Number of negative samples: 5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjUU4MKN1c-f"
      },
      "source": [
        "**Build the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUNrQfFNRDMh"
      },
      "source": [
        "model1.build_vocab(corpus)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENkBBCTN_5gI",
        "outputId": "d5fbf267-ad98-420c-c75d-9b629d63d1ab"
      },
      "source": [
        "# Get the list of words in the vocabulary\n",
        "words = model1.wv.vocab\n",
        "print('Total number of unique words loaded in Model : ', len(model1.wv.vocab))\n",
        "\n",
        "# Create a corresponding list of the count for each word\n",
        "word_ranks = []\n",
        "for word in words:\n",
        "    word_ranks.append(model1.wv.vocab[word].count)\n",
        "    \n",
        "# Sort both lists by the word counts, descending (most frequent first)\n",
        "word_ranks, words = map(list, zip(*sorted(zip(word_ranks, words), reverse=True)))\n",
        "\n",
        "print('\\nThe 30 most frequest words:\\n')\n",
        "print('    ---Rank---  ---Word---')\n",
        "\n",
        "# For the 30 most frequent words\n",
        "for i in range(30):\n",
        "    # Print the rank with commas and pad it to 12 characters.\n",
        "    print('{:>12,}     {:}'.format(word_ranks[i], words[i]))\n",
        "\n",
        "print('\\nThe 30 least frequest words:\\n')\n",
        "print('    ---Rank---   ---Word---')\n",
        "\n",
        "# Go backwards through the last 10 indeces\n",
        "for i in range(-1, -30, -1):\n",
        "    # Print the count with commas and pad it to 12 characters.\n",
        "    print('{:>12,}     {:}'.format(word_ranks[i], words[i]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words loaded in Model :  25037\n",
            "\n",
            "The 30 most frequest words:\n",
            "\n",
            "    ---Rank---  ---Word---\n",
            "      49,823     hotel\n",
            "      35,341     room\n",
            "      21,477     great\n",
            "      19,102     n\n",
            "      17,417     good\n",
            "      16,637     staff\n",
            "      15,413     stay\n",
            "      12,644     nice\n",
            "      12,406     rooms\n",
            "      11,357     location\n",
            "      10,502     stayed\n",
            "      10,371     service\n",
            "      10,157     night\n",
            "      10,122     time\n",
            "      10,065     beach\n",
            "       9,976     day\n",
            "       9,738     breakfast\n",
            "       9,597     clean\n",
            "       9,415     food\n",
            "       8,254     like\n",
            "       8,141     resort\n",
            "       7,791     place\n",
            "       7,790     really\n",
            "       7,578     pool\n",
            "       6,893     friendly\n",
            "       6,839     people\n",
            "       6,595     small\n",
            "       6,260     little\n",
            "       6,255     walk\n",
            "       6,206     got\n",
            "\n",
            "The 30 least frequest words:\n",
            "\n",
            "    ---Rank---   ---Word---\n",
            "           2     aahh\n",
            "           2     aand\n",
            "           2     aas\n",
            "           2     abandon\n",
            "           2     abandoning\n",
            "           2     abeautiful\n",
            "           2     aber\n",
            "           2     aberdeen\n",
            "           2     abolutely\n",
            "           2     abominable\n",
            "           2     aboout\n",
            "           2     aborted\n",
            "           2     abraham\n",
            "           2     abruptly\n",
            "           2     absoluetly\n",
            "           2     absoluteley\n",
            "           2     absolutey\n",
            "           2     absorbant\n",
            "           2     absoulte\n",
            "           2     absoultly\n",
            "           2     absoutly\n",
            "           2     abstract\n",
            "           2     abundantly\n",
            "           2     acadamia\n",
            "           2     academie\n",
            "           2     accademie\n",
            "           2     accedemia\n",
            "           2     accented\n",
            "           2     accentuate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2SW54O727oG"
      },
      "source": [
        "**Train the Word2Vec model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqYh_d19Re8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6245cb5-6bcf-46fd-b18a-084ff3fc55e9"
      },
      "source": [
        "model1.train(sentences=corpus, total_examples=len(corpus), epochs=model1.iter)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9138370, 10143600)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN500v-1R8YR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd318eec-5319-43f4-b7d1-2233aaf43cbc"
      },
      "source": [
        "word_vectors = model1.wv\n",
        "print('The vocabulary includes {} unique words.'.format(len(word_vectors.vocab)))\n",
        "vector = model1.wv['nice']\n",
        "print('The length of a word vector according to the parameter \"size\": ',len(vector))\n",
        "print('Numpy vector of the word \"nice\":\\n',vector)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary includes 25037 unique words.\n",
            "The length of a word vector according to the parameter \"size\":  100\n",
            "Numpy vector of the word \"nice\":\n",
            " [-0.16406265  0.23530467 -0.29546687  0.17013428  0.26952535  0.12750848\n",
            "  0.3784242  -0.3520841  -0.42471138  0.39322042  0.01476069  0.55951273\n",
            "  0.1652967   0.3257624   0.12523626 -0.16914952 -0.13219458  0.7724101\n",
            " -0.11426571 -0.03218948 -0.26684275  0.32217103  0.40112337 -0.19855571\n",
            "  0.21553007 -0.04242864  0.32383937 -0.0914438  -0.10170554 -0.14913952\n",
            "  0.18517165  0.08040129 -0.19676238 -0.23305827  0.03726035 -0.10574959\n",
            "  0.13444589  0.2228283  -0.08959322  0.0409127  -0.4797448  -0.32173702\n",
            " -0.10882694  0.09740351  0.14658293  0.2032278   0.5547828  -0.32607338\n",
            " -0.23868999  0.2789851   0.25150564  0.10932046 -0.28914598 -0.0990825\n",
            " -0.5564771   0.43575716 -0.3317961   0.30235058 -0.47585264 -0.28579777\n",
            "  0.00349328 -0.17990397  0.03916128  0.09382967 -0.2797046   0.14110936\n",
            " -0.09691735 -0.07747447  0.36744773 -0.32765758  0.27545473  0.21724644\n",
            "  0.46412995 -0.03065855  0.47430706 -0.22575305 -0.42912227  0.14197128\n",
            " -0.28016627  0.24930926  0.23688833 -0.02516021  0.02765956 -0.19369768\n",
            "  0.32610762  0.14322075  0.38414687  0.39286828  0.35235247 -0.17904128\n",
            " -0.3724529   0.27484065 -0.32346103 -0.26711974 -0.04219569  0.45176637\n",
            " -0.15399851 -0.1381513   0.15708773  0.33322614]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj_8FvEQ4oqG"
      },
      "source": [
        "**Find the most similar words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcqc9XA8Wlo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dad3c00-bded-4166-d7a8-526d35073ce6"
      },
      "source": [
        "print('Most similar words for \"expensive\"\\n')\n",
        "model1.wv.most_similar('expensive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"expensive\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pricey', 0.867865264415741),\n",
              " ('pricy', 0.8189133405685425),\n",
              " ('costly', 0.8106584548950195),\n",
              " ('overpriced', 0.8106163740158081),\n",
              " ('cheaper', 0.7656259536743164),\n",
              " ('pricier', 0.7643506526947021),\n",
              " ('costs', 0.7536859512329102),\n",
              " ('outrageous', 0.7531921863555908),\n",
              " ('inflated', 0.7527035474777222),\n",
              " ('priced', 0.7451817989349365)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGAd05BQWnyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2b7780-68b1-4ca8-feb0-a0d25262f18d"
      },
      "source": [
        "print('Most similar words for \"great\"\\n')\n",
        "model1.wv.most_similar('great')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"great\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrific', 0.8998247385025024),\n",
              " ('fantastic', 0.8552649021148682),\n",
              " ('phenomenal', 0.8421623706817627),\n",
              " ('excellent', 0.8375718593597412),\n",
              " ('good', 0.8166176080703735),\n",
              " ('brilliant', 0.8117712736129761),\n",
              " ('excellant', 0.8039320707321167),\n",
              " ('awesome', 0.8012272119522095),\n",
              " ('wonderful', 0.7968956232070923),\n",
              " ('tremendous', 0.7966464757919312)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDQM9dHuBEoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78c1ef9-bec7-4cee-8859-95fbdbdac132"
      },
      "source": [
        "print('Most similar words for \"bad\"\\n')\n",
        "model1.wv.most_similar('bad')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"bad\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('horrible', 0.7663941383361816),\n",
              " ('terrible', 0.7481241226196289),\n",
              " ('sucks', 0.7375690937042236),\n",
              " ('dreadful', 0.7321875095367432),\n",
              " ('kidding', 0.7221542596817017),\n",
              " ('awful', 0.7106975317001343),\n",
              " ('lousy', 0.7034407258033752),\n",
              " ('darn', 0.698879599571228),\n",
              " ('critical', 0.697730302810669),\n",
              " ('complains', 0.6964166760444641)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMZ2iHBz_94i"
      },
      "source": [
        "**Compare similarity of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iGQSGXKVhBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20242aef-cfeb-43a9-9f47-7a6719132927"
      },
      "source": [
        "# Similarity between two words\n",
        "score1 = model1.similarity('expensive', 'good')\n",
        "print('Cosine similarity between \"expensive\" and \"good\" is: %.2f\\n' % score1)\n",
        "\n",
        "score2 = model1.similarity('nice', 'good')\n",
        "print('Cosine similarity between \"nice\" and \"good\" is: %.2f\\n' % score2)\n",
        "\n",
        "# Similarity between lists of words\n",
        "print('Similarity between the lists: ',model1.wv.n_similarity(['beautiful','lovely'],['sucks','awful']))\n",
        "print('\\nSimilarity between the lists: ',model1.wv.n_similarity(['beautiful','lovely'],['nice','great']))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between \"expensive\" and \"good\" is: 0.50\n",
            "\n",
            "Cosine similarity between \"nice\" and \"good\" is: 0.68\n",
            "\n",
            "Similarity between the lists:  0.44613847\n",
            "\n",
            "Similarity between the lists:  0.795597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL1yfR7dBrYm",
        "outputId": "08b4000f-3b63-445b-fffe-d1d09e4b1a57"
      },
      "source": [
        "# Calculate distance between words\n",
        "print('Distance between \"terrible\" and \"horrible\": ', model1.wv.distance('terrible','horrible'))\n",
        "print('Distance between \"friendly\" and \"nice\": ', model1.wv.distance('friendly','nice'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between \"terrible\" and \"horrible\":  0.11778444051742554\n",
            "Distance between \"friendly\" and \"nice\":  0.4149606227874756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_HHBAYeNl8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c8c0d7-2738-4c35-f8c6-a5e9f31e9928"
      },
      "source": [
        "# Find words that do not match\n",
        "print('Word that does not match with the rest: ',model1.wv.doesnt_match(['brilliant','service','fabulous']))\n",
        "\n",
        "# Find the most similar words by using the default \"cosine similarity\" measure\n",
        "cos_sim1 = model1.wv.most_similar(positive=['nice', 'friendly'], negative=['awful'])\n",
        "most_similar_key, similarity = cos_sim1[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "cos_sim2 = model1.wv.similar_by_word('terrible')\n",
        "most_similar_key, similarity = cos_sim2[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word that does not match with the rest:  service\n",
            "The most similar word is \"courteous\": 0.6560\n",
            "The most similar word is \"horrible\": 0.8822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjCpRiWonYvC"
      },
      "source": [
        "**WORD2VEC MODEL 2 CBOW**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo830-X3ZcOv"
      },
      "source": [
        "**Set model parameteres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XILXeOlFZPmC"
      },
      "source": [
        "model2 = gensim.models.Word2Vec(size= 100, window = 3, min_count = 1, sg = 0, workers = 5, hs = 0, negative = 5)\n",
        "\n",
        "# Size of dimensionality of word vectors: 100\n",
        "# Window, the maximum distance between the target word and its neighboring word: 3\n",
        "# Minimum count, ignoring all words with total frequency lower than this, words must appear this many times to be in vocab: 1\n",
        "# Sg, training algorithm: CBOW\n",
        "# The number of worker threads used to train the model: 5\n",
        "# 0: Negative sampling\n",
        "# Number of negative samples: 5"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP5zdGPvZnMK"
      },
      "source": [
        "**Build the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvJcfTTNZnMK"
      },
      "source": [
        "model2.build_vocab(corpus)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixFtZX0ZnMK"
      },
      "source": [
        "**Train the second Word2Vec model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUiznqaFZnML",
        "outputId": "fcba9971-500e-442b-e1f1-c194775368a4"
      },
      "source": [
        "model2.train(sentences=corpus, total_examples=len(corpus), epochs=model2.iter)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9267278, 10143600)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBZIp9YgZnML",
        "outputId": "49d7ea21-ab2f-4100-bc78-583ba1cdd055"
      },
      "source": [
        "word_vectors2 = model2.wv\n",
        "print('The vocabulary includes {} unique words.'.format(len(word_vectors2.vocab)))\n",
        "vector2 = model2.wv['nice']\n",
        "print('The length of a word vector according to the parameter \"size\": ',len(vector2))\n",
        "print('Numpy vector of the word \"nice\":\\n',vector2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary includes 48959 unique words.\n",
            "The length of a word vector according to the parameter \"size\":  100\n",
            "Numpy vector of the word \"nice\":\n",
            " [ 0.09360968  0.14551876  1.0759455   1.7566912   1.5321697   1.1960722\n",
            "  0.2295063  -1.4187635  -1.4068594  -0.26937306 -1.1585968  -0.06155514\n",
            "  0.87846786  0.6588273  -0.31286204 -0.97868735 -0.26431593  0.9983019\n",
            "  0.04461127 -0.07919632 -1.4657954   0.97029173 -0.56872743  0.08821569\n",
            " -0.7368466   0.20135616  0.41127786  0.21696232 -0.18049097  0.04047824\n",
            "  0.751256   -0.5013633  -0.24166001 -0.4304393   1.0530143  -0.8651481\n",
            "  0.16809657 -0.03175985  0.39211163 -0.47118062  0.11397498 -1.3893498\n",
            " -0.39323118 -0.1871048   0.63553977  1.7483108   2.4457295  -0.7415914\n",
            " -0.35399443  0.42490858  0.61088824  0.3313907  -0.23141788 -0.824675\n",
            " -0.21933584 -0.6052369  -2.4118636   0.76665354 -0.6808813  -0.92777485\n",
            " -0.30381238 -1.3019224   0.69423383  0.10692751 -1.0556427   1.2760147\n",
            " -0.63577783 -0.7631183   0.8720977  -1.0272728  -0.5324697   1.1423789\n",
            "  1.0023358  -1.6967787   1.0084081  -1.5120045  -0.34569392  1.1670823\n",
            "  0.41390795  0.24001151  0.4874685  -1.3189179   0.23660167 -1.074045\n",
            " -0.6171489   0.70180726  0.1787668   1.1147158   1.5942956  -0.36700517\n",
            " -1.2133402  -0.08400851 -0.89529777 -0.8070923  -0.16093595  0.32672945\n",
            " -0.10387     0.05145498 -0.06097882  0.9519017 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWC92sNenHLs"
      },
      "source": [
        "**Find the most similar words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBZo69Z7nHLt",
        "outputId": "d7676b45-7870-4c99-ede6-6272bb83c64d"
      },
      "source": [
        "print('Most similar words for \"expensive\"\\n')\n",
        "model2.wv.most_similar('expensive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"expensive\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pricey', 0.9085597991943359),\n",
              " ('overpriced', 0.8676849603652954),\n",
              " ('inexpensive', 0.760445773601532),\n",
              " ('cheaper', 0.7571960687637329),\n",
              " ('pricy', 0.7550462484359741),\n",
              " ('cheap', 0.7473670244216919),\n",
              " ('prices', 0.714169979095459),\n",
              " ('option', 0.7082355618476868),\n",
              " ('reasonable', 0.6970846652984619),\n",
              " ('fair', 0.6960881948471069)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LWCm9zNnHLt",
        "outputId": "ded3c3c8-2f38-4cc1-a67c-d7ab310241aa"
      },
      "source": [
        "print('Most similar words for \"great\"\\n')\n",
        "model2.wv.most_similar('great')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"great\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fantastic', 0.891322135925293),\n",
              " ('excellent', 0.8648792505264282),\n",
              " ('terrific', 0.8608647584915161),\n",
              " ('brilliant', 0.8455147743225098),\n",
              " ('wonderful', 0.8205458521842957),\n",
              " ('awesome', 0.8165668249130249),\n",
              " ('fabulous', 0.8154141902923584),\n",
              " ('good', 0.8147371411323547),\n",
              " ('amazing', 0.7870365381240845),\n",
              " ('superb', 0.786649227142334)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnuJ4O4YGwWB",
        "outputId": "1f9ddd58-d740-42cb-ccb0-d3c88a6b14b8"
      },
      "source": [
        "print('Most similar words for \"bad\"\\n')\n",
        "model2.wv.most_similar('bad')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"bad\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrible', 0.7914304733276367),\n",
              " ('horrible', 0.768926739692688),\n",
              " ('poor', 0.7583184242248535),\n",
              " ('awful', 0.7541587352752686),\n",
              " ('ok', 0.7479483485221863),\n",
              " ('okay', 0.7346817255020142),\n",
              " ('negative', 0.7130692005157471),\n",
              " ('worse', 0.7113240957260132),\n",
              " ('complain', 0.7010729312896729),\n",
              " ('thats', 0.6883893609046936)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz6YyIW4nHLt"
      },
      "source": [
        "**Compare similarity of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6gABrWlnHLt",
        "outputId": "240dfc26-3f2b-4dae-91b6-3edf85bff1e7"
      },
      "source": [
        "# Similarity between two words\n",
        "score_1 = model2.similarity('expensive', 'good')\n",
        "print('Cosine similarity between \"expensive\" and \"good\" is: %.2f\\n' % score_1)\n",
        "\n",
        "score_2 = model2.similarity('nice', 'good')\n",
        "print('Cosine similarity between \"nice\" and \"good\" is: %.2f\\n' % score_2)\n",
        "\n",
        "# Similarity between lists of words\n",
        "print('Similarity between the lists: ',model2.wv.n_similarity(['beautiful','lovely'],['sucks','awful']))\n",
        "print('\\nSimilarity between the lists: ',model2.wv.n_similarity(['beautiful','lovely'],['nice','great']))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between \"expensive\" and \"good\" is: 0.49\n",
            "\n",
            "Cosine similarity between \"nice\" and \"good\" is: 0.60\n",
            "\n",
            "Similarity between the lists:  0.2962033\n",
            "\n",
            "Similarity between the lists:  0.7642074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74mf7tWHnHLt",
        "outputId": "256b1824-07e7-4696-ba63-426b08697b86"
      },
      "source": [
        "# Calculate distance between words\n",
        "print('Distance between \"terrible\" and \"horrible\": ', model2.wv.distance('terrible','horrible'))\n",
        "print('Distance between \"friendly\" and \"nice\": ', model2.wv.distance('friendly','nice'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between \"terrible\" and \"horrible\":  0.048496782779693604\n",
            "Distance between \"friendly\" and \"nice\":  0.418826699256897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aTELbCEnHLt",
        "outputId": "e414d2e6-3072-4e82-f2b5-9a92c0ee64a5"
      },
      "source": [
        "# Find words that do not match\n",
        "print('Word that does not match with the rest: ',model2.wv.doesnt_match(['brilliant','service','fabulous']))\n",
        "\n",
        "# Find the most similar words by using the default \"cosine similarity\" measure\n",
        "cos_sim_1 = model2.wv.most_similar(positive=['nice', 'friendly'], negative=['awful'])\n",
        "most_similar_key, similarity = cos_sim_1[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "cos_sim_2 = model2.wv.similar_by_word('terrible')\n",
        "most_similar_key, similarity = cos_sim_2[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word that does not match with the rest:  service\n",
            "The most similar word is \"pleasant\": 0.7097\n",
            "The most similar word is \"horrible\": 0.9515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpqUIljpHb79"
      },
      "source": [
        "**WORD2VEC MODEL 3 SKIP-GRAM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPkygwDOHb8C"
      },
      "source": [
        "**Set model parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBLaPnXmHb8C"
      },
      "source": [
        "model3 = gensim.models.Word2Vec(size= 50, window = 3, min_count = 2, sg = 1, workers = 15, hs = 0, negative = 5)\n",
        "\n",
        "# Size of dimensionality of word vectors: 50\n",
        "# Window, the maximum distance between the target word and its neighboring word: 3\n",
        "# Minimum count, ignoring all words with total frequency lower than this, words must appear this many times to be in vocab: 2\n",
        "# Sg, training algorithm: Skip-Gram\n",
        "# The number of worker threads used to train the model: 15\n",
        "# 0: Negative sampling\n",
        "# Number of negative samples: 5"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7EJlkisHb8C"
      },
      "source": [
        "model3.build_vocab(corpus)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImhyDhOQ-gt7"
      },
      "source": [
        "**Train the third Word2Vec model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqkKej5CHb8D",
        "outputId": "9867d81c-5629-45b1-c855-91d3907e45ec"
      },
      "source": [
        "model3.train(sentences=corpus, total_examples=len(corpus), epochs=model1.iter)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9137497, 10143600)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb5uBmTqHb8D",
        "outputId": "1511ef4b-cbf7-4b10-c029-6f8ee0d44639"
      },
      "source": [
        "word_vectors = model3.wv\n",
        "print('The vocabulary includes {} unique words.'.format(len(word_vectors.vocab)))\n",
        "vector = model3.wv['nice']\n",
        "print('The length of a word vector according to the parameter \"size\": ',len(vector))\n",
        "print('Numpy vector of the word \"nice\":\\n',vector)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary includes 25037 unique words.\n",
            "The length of a word vector according to the parameter \"size\":  50\n",
            "Numpy vector of the word \"nice\":\n",
            " [ 4.00392711e-03  1.05703533e-01 -9.91758332e-02  3.60172629e-01\n",
            " -1.18538998e-01  2.26613432e-01  4.45170790e-01 -1.37967721e-01\n",
            " -5.91202676e-01  4.62506525e-02 -1.62097692e-01  4.69654292e-01\n",
            "  1.39992833e-01  2.08870620e-01  1.17432944e-01 -3.54794294e-01\n",
            " -5.33169746e-01  9.03924227e-01 -1.55803129e-01 -2.71646321e-01\n",
            " -1.62918955e-01  3.96452516e-01  2.25625187e-01 -3.63199413e-01\n",
            "  1.82856366e-01 -2.70609468e-01  1.65584609e-01 -2.31976509e-01\n",
            "  8.82378139e-04  2.22830296e-01  2.50813842e-01 -8.83651674e-02\n",
            " -1.39806315e-01 -3.84776264e-01  3.47818851e-01 -4.52600211e-01\n",
            "  7.62868822e-02  1.65184349e-01  3.34493697e-01 -3.45065355e-01\n",
            " -3.53405625e-01 -5.52335739e-01 -2.55021542e-01  2.25873664e-02\n",
            "  4.80243206e-01  3.48123640e-01  1.04790926e+00 -5.33670902e-01\n",
            " -4.34281342e-02  2.46509537e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSStUR22-oQf"
      },
      "source": [
        "**Find the most similar words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37yzXT0tHb8D",
        "outputId": "025a188c-534e-4cab-f660-0392977f8a09"
      },
      "source": [
        "print('Most similar words for \"expensive\"\\n')\n",
        "model3.wv.most_similar('expensive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"expensive\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pricey', 0.918716311454773),\n",
              " ('overpriced', 0.8920618295669556),\n",
              " ('pricy', 0.8858973383903503),\n",
              " ('costly', 0.8527936339378357),\n",
              " ('cheaper', 0.8526806831359863),\n",
              " ('inflated', 0.8352290987968445),\n",
              " ('pricier', 0.8297056555747986),\n",
              " ('competitive', 0.817486584186554),\n",
              " ('prices', 0.8169474005699158),\n",
              " ('inexpensive', 0.8158344030380249)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YbF2DI6Hb8D",
        "outputId": "9be2605c-0e9d-497e-842b-0d9e8fda3d7c"
      },
      "source": [
        "print('Most similar words for \"great\"\\n')\n",
        "model3.wv.most_similar('great')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"great\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fantastic', 0.9265666604042053),\n",
              " ('excellent', 0.9168330430984497),\n",
              " ('terrific', 0.9165868759155273),\n",
              " ('brilliant', 0.905572772026062),\n",
              " ('excellant', 0.8913705945014954),\n",
              " ('good', 0.8794829249382019),\n",
              " ('geat', 0.8719421029090881),\n",
              " ('wonderful', 0.8678866028785706),\n",
              " ('tremendous', 0.8653309941291809),\n",
              " ('fabulous', 0.854750394821167)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cFtTMC_Hb8D",
        "outputId": "7508069f-376b-4d8f-e4f5-329722438358"
      },
      "source": [
        "print('Most similar words for \"bad\"\\n')\n",
        "model3.wv.most_similar('bad')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"bad\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrible', 0.8347867131233215),\n",
              " ('horrible', 0.8318676948547363),\n",
              " ('awful', 0.8252952694892883),\n",
              " ('sucks', 0.8163849711418152),\n",
              " ('horrific', 0.8122080564498901),\n",
              " ('thats', 0.8046402335166931),\n",
              " ('downer', 0.8035178780555725),\n",
              " ('kidding', 0.8033908605575562),\n",
              " ('lousy', 0.8010213971138),\n",
              " ('darn', 0.7956486344337463)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMHxQ9QwHb8D"
      },
      "source": [
        "**Compare similarity of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TTHZreaHb8D",
        "outputId": "aebee6fb-c3f5-4316-8a03-0734a2564912"
      },
      "source": [
        "# Similarity between two words\n",
        "score__1 = model3.similarity('expensive', 'good')\n",
        "print('Cosine similarity between \"expensive\" and \"good\" is: %.2f\\n' % score1)\n",
        "\n",
        "score__2 = model3.similarity('nice', 'good')\n",
        "print('Cosine similarity between \"nice\" and \"good\" is: %.2f\\n' % score2)\n",
        "\n",
        "# Similarity between lists of words\n",
        "print('Similarity between the lists: ',model3.wv.n_similarity(['beautiful','lovely'],['sucks','awful']))\n",
        "print('\\nSimilarity between the lists: ',model3.wv.n_similarity(['beautiful','lovely'],['nice','great']))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between \"expensive\" and \"good\" is: 0.50\n",
            "\n",
            "Cosine similarity between \"nice\" and \"good\" is: 0.68\n",
            "\n",
            "Similarity between the lists:  0.5071869\n",
            "\n",
            "Similarity between the lists:  0.8593899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtdQezNcHb8D",
        "outputId": "5e3a8d59-ae3d-4d6d-c271-c2f131a80c4c"
      },
      "source": [
        "# Calculate distance between words\n",
        "print('Distance between \"terrible\" and \"horrible\": ', model3.wv.distance('terrible','horrible'))\n",
        "print('Distance between \"friendly\" and \"nice\": ', model3.wv.distance('friendly','nice'))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between \"terrible\" and \"horrible\":  0.06601947546005249\n",
            "Distance between \"friendly\" and \"nice\":  0.2544809579849243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlH-eUy4Hb8E",
        "outputId": "6402c927-135a-4618-c20b-976b26bbe3e1"
      },
      "source": [
        "# Find words that do not match\n",
        "print('Word that does not match with the rest: ',model3.wv.doesnt_match(['brilliant','service','fabulous']))\n",
        "\n",
        "# Find the most similar words by using the default \"cosine similarity\" measure\n",
        "cos_sim__1 = model3.wv.most_similar(positive=['nice', 'friendly'], negative=['awful'])\n",
        "most_similar_key, similarity = cos_sim__1[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "cos_sim__2 = model3.wv.similar_by_word('terrible')\n",
        "most_similar_key, similarity = cos_sim__2[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word that does not match with the rest:  service\n",
            "The most similar word is \"courteous\": 0.7571\n",
            "The most similar word is \"horrible\": 0.9340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yBh9nw7whCE"
      },
      "source": [
        "**COMPARING RESULTS OF THE 3 MODELS INCLUDING COMMENTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDjnw-j_w28-",
        "outputId": "776a31a7-05e9-47fe-bb7d-03a844072c13"
      },
      "source": [
        "print('Most similar words for \"expensive\"\\n')\n",
        "print('Model 1')\n",
        "print(model1.wv.most_similar('expensive'))\n",
        "\n",
        "print('Model 2')\n",
        "print(model2.wv.most_similar('expensive'))\n",
        "\n",
        "print('Model 3')\n",
        "print(model3.wv.most_similar('expensive'))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"expensive\"\n",
            "\n",
            "Model 1\n",
            "[('pricey', 0.867865264415741), ('pricy', 0.8189133405685425), ('costly', 0.8106584548950195), ('overpriced', 0.8106163740158081), ('cheaper', 0.7656259536743164), ('pricier', 0.7643506526947021), ('costs', 0.7536859512329102), ('outrageous', 0.7531921863555908), ('inflated', 0.7527035474777222), ('priced', 0.7451817989349365)]\n",
            "Model 2\n",
            "[('pricey', 0.9085597991943359), ('overpriced', 0.8676849603652954), ('inexpensive', 0.760445773601532), ('cheaper', 0.7571960687637329), ('pricy', 0.7550462484359741), ('cheap', 0.7473670244216919), ('prices', 0.714169979095459), ('option', 0.7082355618476868), ('reasonable', 0.6970846652984619), ('fair', 0.6960881948471069)]\n",
            "Model 3\n",
            "[('pricey', 0.918716311454773), ('overpriced', 0.8920618295669556), ('pricy', 0.8858973383903503), ('costly', 0.8527936339378357), ('cheaper', 0.8526806831359863), ('inflated', 0.8352290987968445), ('pricier', 0.8297056555747986), ('competitive', 0.817486584186554), ('prices', 0.8169474005699158), ('inexpensive', 0.8158344030380249)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4kbjeosntfL"
      },
      "source": [
        "It seems that the first similar word for \"expensive\" in all models is the word \"pricey\", but it achieves the highest similarity score in model 3. It seems that the similar words for \"expensive\" are ordered differently with various similarity scores, while there are words that may be considered similar for one model, but not similar enough for another (for example, in model 2, the word \"costly\" is not included as most similar to the word \"expensive\" like the other models, the word \"overpriced\" achieved the second higher similarity score in models 2 and 3, but not in model 1). In model 3 the word \"cheaper\" achieves a very high similarity score, which does not seem absolutely correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0QZs4Tow65N",
        "outputId": "d070d0ce-60bf-4360-e045-8d7a4f014420"
      },
      "source": [
        "print('Most similar words for \"great\"\\n')\n",
        "print('Model 1')\n",
        "print(model1.wv.most_similar('great'))\n",
        "\n",
        "print('Model 2')\n",
        "print(model2.wv.most_similar('great'))\n",
        "\n",
        "print('Model 3')\n",
        "print(model3.wv.most_similar('great'))\n",
        "\n",
        "# I think the word \"good\" might have achieved lower similarity score than words like \"brilliant\", \"wonderful\", \"fabulous\", \"awesome\" because it does not seem absolutely right according to vocabulary knowledge."
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"great\"\n",
            "\n",
            "Model 1\n",
            "[('terrific', 0.8998247385025024), ('fantastic', 0.8552649021148682), ('phenomenal', 0.8421623706817627), ('excellent', 0.8375718593597412), ('good', 0.8166176080703735), ('brilliant', 0.8117712736129761), ('excellant', 0.8039320707321167), ('awesome', 0.8012272119522095), ('wonderful', 0.7968956232070923), ('tremendous', 0.7966464757919312)]\n",
            "Model 2\n",
            "[('fantastic', 0.891322135925293), ('excellent', 0.8648792505264282), ('terrific', 0.8608647584915161), ('brilliant', 0.8455147743225098), ('wonderful', 0.8205458521842957), ('awesome', 0.8165668249130249), ('fabulous', 0.8154141902923584), ('good', 0.8147371411323547), ('amazing', 0.7870365381240845), ('superb', 0.786649227142334)]\n",
            "Model 3\n",
            "[('fantastic', 0.9265666604042053), ('excellent', 0.9168330430984497), ('terrific', 0.9165868759155273), ('brilliant', 0.905572772026062), ('excellant', 0.8913705945014954), ('good', 0.8794829249382019), ('geat', 0.8719421029090881), ('wonderful', 0.8678866028785706), ('tremendous', 0.8653309941291809), ('fabulous', 0.854750394821167)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTRVYGMdnKNx"
      },
      "source": [
        "In model 1 the word \"terrific\" achieved the highest similarity score, while in the other two models the word \"fantastic\". Models 2 and 3 seem to have more similar results up to one point, whereas model 1 presents completely different results. I think the word \"good\" should have achieved a lower similarity score than words like \"brilliant\", \"wonderful\", \"fabulous\", \"awesome\" in all models, as it does not seem absolutely correct for someone who is aware of the meaning of the particular vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rVLWU7Lwe-O",
        "outputId": "34c68d3b-f5d9-413c-c5ee-b1651f0e36d2"
      },
      "source": [
        "print('Most similar words for \"bad\"\\n')\n",
        "print('Model 1')\n",
        "print(model1.wv.most_similar('bad'))\n",
        "\n",
        "print('Model 2')\n",
        "print(model2.wv.most_similar('bad'))\n",
        "\n",
        "print('Model 3')\n",
        "print(model3.wv.most_similar('bad'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words for \"bad\"\n",
            "\n",
            "Model 1\n",
            "[('horrible', 0.7663941383361816), ('terrible', 0.7481241226196289), ('sucks', 0.7375690937042236), ('dreadful', 0.7321875095367432), ('kidding', 0.7221542596817017), ('awful', 0.7106975317001343), ('lousy', 0.7034407258033752), ('darn', 0.698879599571228), ('critical', 0.697730302810669), ('complains', 0.6964166760444641)]\n",
            "Model 2\n",
            "[('terrible', 0.7914304733276367), ('horrible', 0.768926739692688), ('poor', 0.7583184242248535), ('awful', 0.7541587352752686), ('ok', 0.7479483485221863), ('okay', 0.7346817255020142), ('negative', 0.7130692005157471), ('worse', 0.7113240957260132), ('complain', 0.7010729312896729), ('thats', 0.6883893609046936)]\n",
            "Model 3\n",
            "[('terrible', 0.8347867131233215), ('horrible', 0.8318676948547363), ('awful', 0.8252952694892883), ('sucks', 0.8163849711418152), ('horrific', 0.8122080564498901), ('thats', 0.8046402335166931), ('downer', 0.8035178780555725), ('kidding', 0.8033908605575562), ('lousy', 0.8010213971138), ('darn', 0.7956486344337463)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHOlB9sLnAUy"
      },
      "source": [
        "It seems that model 1 scores higher the word \"horrible\", while models 2 and 3 score higher the word \"terrible\". Each model orders differently the score similarities of the word \"bad\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-lxQD6IxG69",
        "outputId": "0401d54b-95da-4fc5-c443-0d559d6dec24"
      },
      "source": [
        "print('\\nModel 1\\n')\n",
        "# Similarity between two words for model 1\n",
        "score1 = model1.similarity('expensive', 'good')\n",
        "print('Cosine similarity between \"expensive\" and \"good\" is: %.2f\\n' % score1)\n",
        "\n",
        "score2 = model1.similarity('nice', 'good')\n",
        "print('Cosine similarity between \"nice\" and \"good\" is: %.2f\\n' % score2)\n",
        "\n",
        "# Similarity between lists of words for model 1\n",
        "print('Similarity between the pairs \"beautiful-lovely\" and \"sucks-awful\": ',model1.wv.n_similarity(['beautiful','lovely'],['sucks','awful']))\n",
        "print('\\nSimilarity between the pairs \"beautiful-lovely\" and \"nice-great\": ',model1.wv.n_similarity(['beautiful','lovely'],['nice','great']))\n",
        "\n",
        "print('\\nModel 2\\n')\n",
        "# Similarity between two words for model 2\n",
        "score_1 = model2.similarity('expensive', 'good')\n",
        "print('Cosine similarity between \"expensive\" and \"good\" is: %.2f\\n' % score_1)\n",
        "\n",
        "score_2 = model2.similarity('nice', 'good')\n",
        "print('Cosine similarity between \"nice\" and \"good\" is: %.2f\\n' % score_2)\n",
        "\n",
        "# Similarity between lists of words for model 2\n",
        "print('Similarity between the \"beautiful-lovely\" and \"sucks-awful\": ',model2.wv.n_similarity(['beautiful','lovely'],['sucks','awful']))\n",
        "print('\\nSimilarity between the pairs \"beautiful-lovely\" and \"nice-great\": ',model2.wv.n_similarity(['beautiful','lovely'],['nice','great']))\n",
        "\n",
        "print('\\nModel 3\\n')\n",
        "# Similarity between two words for model 3\n",
        "score__1 = model3.similarity('expensive', 'good')\n",
        "print('Cosine similarity between \"expensive\" and \"good\" is: %.2f\\n' % score__1)\n",
        "\n",
        "score__2 = model3.similarity('nice', 'good')\n",
        "print('Cosine similarity between \"nice\" and \"good\" is: %.2f\\n' % score__2)\n",
        "\n",
        "# Similarity between lists of words for model 3\n",
        "print('Similarity between the \"beautiful-lovely\" and \"sucks-awful\": ',model3.wv.n_similarity(['beautiful','lovely'],['sucks','awful']))\n",
        "print('\\nSimilarity between the pairs \"beautiful-lovely\" and \"nice-great\": ',model3.wv.n_similarity(['beautiful','lovely'],['nice','great']))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 1\n",
            "\n",
            "Cosine similarity between \"expensive\" and \"good\" is: 0.50\n",
            "\n",
            "Cosine similarity between \"nice\" and \"good\" is: 0.68\n",
            "\n",
            "Similarity between the pairs \"beautiful-lovely\" and \"sucks-awful\":  0.44613847\n",
            "\n",
            "Similarity between the pairs \"beautiful-lovely\" and \"nice-great\":  0.795597\n",
            "\n",
            "Model 2\n",
            "\n",
            "Cosine similarity between \"expensive\" and \"good\" is: 0.49\n",
            "\n",
            "Cosine similarity between \"nice\" and \"good\" is: 0.60\n",
            "\n",
            "Similarity between the \"beautiful-lovely\" and \"sucks-awful\":  0.2962033\n",
            "\n",
            "Similarity between the pairs \"beautiful-lovely\" and \"nice-great\":  0.7642074\n",
            "\n",
            "Model 3\n",
            "\n",
            "Cosine similarity between \"expensive\" and \"good\" is: 0.58\n",
            "\n",
            "Cosine similarity between \"nice\" and \"good\" is: 0.74\n",
            "\n",
            "Similarity between the \"beautiful-lovely\" and \"sucks-awful\":  0.5071869\n",
            "\n",
            "Similarity between the pairs \"beautiful-lovely\" and \"nice-great\":  0.8593899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZODRkun3pR"
      },
      "source": [
        "For the cosine similarity between the pairs \"expensive-good\", model 2 is more strict and achieved a lower similarity score, which seems more correct than in the other models. Model 3 proved that these words are more than 50% similar, which does not seem right.\n",
        "\n",
        "For the cosine similarity between the pairs \"nice-good\", model 3 achieved the highest score of 74%, which seems correct, while model 2 the lowest (60%).\n",
        "\n",
        "For the similarity between the pairs \"beautiful-lovely\" and \"sucks-awful\", model 2 achieved the lowest score (29.6%), which makes sense since these words are not very similar in meaning, whereas model 3 achieved a score of 50.7%.\n",
        "\n",
        "For the similarity between the pairs \"beautiful-lovely\" and \"nice-great\", model 3 achieved the highest similarity score (86.5%) following model 1 with score 79.2%, which are considered good scores for these pairs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXGri-HvxPg8",
        "outputId": "0afe7cec-b174-479c-d0a2-907ece780619"
      },
      "source": [
        "print('\\nModel 1\\n')\n",
        "# Calculate distance between words\n",
        "print('Distance between \"terrible\" and \"horrible\": ', model1.wv.distance('terrible','horrible'))\n",
        "print('Distance between \"friendly\" and \"nice\": ', model1.wv.distance('friendly','nice'))\n",
        "\n",
        "print('\\nModel 2\\n')\n",
        "# Calculate distance between words\n",
        "print('Distance between \"terrible\" and \"horrible\": ', model2.wv.distance('terrible','horrible'))\n",
        "print('Distance between \"friendly\" and \"nice\": ', model2.wv.distance('friendly','nice'))\n",
        "\n",
        "print('\\nModel 3\\n')\n",
        "# Calculate distance between words\n",
        "print('Distance between \"terrible\" and \"horrible\": ', model3.wv.distance('terrible','horrible'))\n",
        "print('Distance between \"friendly\" and \"nice\": ', model3.wv.distance('friendly','nice'))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 1\n",
            "\n",
            "Distance between \"terrible\" and \"horrible\":  0.11778444051742554\n",
            "Distance between \"friendly\" and \"nice\":  0.4149606227874756\n",
            "\n",
            "Model 2\n",
            "\n",
            "Distance between \"terrible\" and \"horrible\":  0.048496782779693604\n",
            "Distance between \"friendly\" and \"nice\":  0.418826699256897\n",
            "\n",
            "Model 3\n",
            "\n",
            "Distance between \"terrible\" and \"horrible\":  0.06601947546005249\n",
            "Distance between \"friendly\" and \"nice\":  0.2544809579849243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3XtXuFsNDn"
      },
      "source": [
        "Model 2 assigns a distance closer to zero which denotes greater similarity to the pairs \"terrible-horrible\",while model 3 the closer distance to the pair \"friendly-nice\". However, the dictance between the last pair could have been achieved less by the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo3GWSZtxakU",
        "outputId": "ad3b33d2-d5d2-43bb-b3c3-e2b1514898c6"
      },
      "source": [
        "print('\\nModel 1\\n')\n",
        "# Find words that do not match for model 1\n",
        "print('Word that does not match with the rest: ',model1.wv.doesnt_match(['brilliant','service','fabulous']))\n",
        "\n",
        "# Find the most similar words by using the default \"cosine similarity\" measure\n",
        "cos_sim1 = model1.wv.most_similar(positive=['nice', 'friendly'], negative=['awful'])\n",
        "most_similar_key, similarity = cos_sim1[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "cos_sim2 = model1.wv.similar_by_word('terrible')\n",
        "most_similar_key, similarity = cos_sim2[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "print('\\nModel 2\\n')\n",
        "# Find words that do not match for model 2\n",
        "print('Word that does not match with the rest: ',model2.wv.doesnt_match(['brilliant','service','fabulous']))\n",
        "\n",
        "# Find the most similar words by using the default \"cosine similarity\" measure\n",
        "cos_sim_1 = model2.wv.most_similar(positive=['nice', 'friendly'], negative=['awful'])\n",
        "most_similar_key, similarity = cos_sim_1[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "cos_sim_2 = model2.wv.similar_by_word('terrible')\n",
        "most_similar_key, similarity = cos_sim_2[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "print('\\nModel 3\\n')\n",
        "# Find words that do not match for model 3\n",
        "print('Word that does not match with the rest: ',model3.wv.doesnt_match(['brilliant','service','fabulous']))\n",
        "\n",
        "# Find the most similar words by using the default \"cosine similarity\" measure\n",
        "cos_sim_3 = model3.wv.most_similar(positive=['nice', 'friendly'], negative=['awful'])\n",
        "most_similar_key, similarity = cos_sim_3[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')\n",
        "\n",
        "cos_sim_4 = model3.wv.similar_by_word('terrible')\n",
        "most_similar_key, similarity = cos_sim_4[0]  # look at the first match\n",
        "print(f'The most similar word is \"{most_similar_key}\": {similarity:.4f}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 1\n",
            "\n",
            "Word that does not match with the rest:  service\n",
            "The most similar word is \"courteous\": 0.6560\n",
            "The most similar word is \"horrible\": 0.8822\n",
            "\n",
            "Model 2\n",
            "\n",
            "Word that does not match with the rest:  service\n",
            "The most similar word is \"pleasant\": 0.7097\n",
            "The most similar word is \"horrible\": 0.9515\n",
            "\n",
            "Model 3\n",
            "\n",
            "Word that does not match with the rest:  service\n",
            "The most similar word is \"courteous\": 0.7571\n",
            "The most similar word is \"horrible\": 0.9340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrAiww0JxXfm"
      },
      "source": [
        "All three models successfully managed to distinguish the word that does not match the set of vocabulary given. \n",
        "\n",
        "Given positive and negative words, one word was found that is similar to the positive words and opposite to the negative word. It seems that in models 1 and 3 the word \"courteous\" is more similar to the words \"nice\" and \"friendly\" and more opposite to the word \"awful\" with model 3 achieving greater score. However, model 2 came up with a different word as result. In model 2, the word \"pleasant\" was found with a score of 70.9%.\n",
        "\n",
        "It seems that in all the three models the word \"horrible\" is more similar to the word \"terrible\", with model 2 achieving the highest similarity score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-gY6bLRL27u"
      },
      "source": [
        "**OVERVIEW: CHARACTERISTICS OF MODELS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZQ75c-oZ0QJ"
      },
      "source": [
        "**Model 1**\n",
        "\n",
        "Training algorithm: Skip-Gram\n",
        "\n",
        "Size of dimensionality of word vectors: 100\n",
        "\n",
        "Window, the maximum distance between the target word and its neighboring word: 3\n",
        "\n",
        "Minimum count, ignoring all words with total frequency lower than this, words must appear this many times to be in vocab: 2\n",
        "\n",
        "The number of worker threads used to train the model: 10\n",
        "\n",
        "Negative sampling, Number of negative samples: 5\n",
        "\n",
        "**Model 2**\n",
        "\n",
        "Training algorithm: CBOW\n",
        "\n",
        "Size of dimensionality of word vectors: 100\n",
        "\n",
        "Window, the maximum distance between the target word and its neighboring word: 3\n",
        "\n",
        "Minimum count, ignoring all words with total frequency lower than this, words must appear this many times to be in vocab: 1\n",
        "\n",
        "The number of worker threads used to train the model: 5\n",
        "\n",
        "Negative sampling, Number of negative samples: 5\n",
        "\n",
        "**Model 3**\n",
        "\n",
        "Training algorithm: Skip-Gram\n",
        "\n",
        "Size of dimensionality of word vectors: 50\n",
        "\n",
        "Window, the maximum distance between the target word and its neighboring word: 3\n",
        "\n",
        "Minimum count, ignoring all words with total frequency lower than this, words must appear this many times to be in vocab: 2\n",
        "\n",
        "The number of worker threads used to train the model: 15\n",
        "\n",
        "Negative sampling, Number of negative samples: 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzZw9Oc0fuD"
      },
      "source": [
        "Models 1 and 3 are Skip-Gram Word2Vec models, meaning that they take as input one word and return as output multiple words according to the window size (3). Model 2 is a CBOW Word2Vec model, meaning that it takes the context of each word as the input and tries to predict the word corresponding to the context.  Multiple words are used as input per window size, but it returns one word as output. Apart from the type of training model, I experimented with the size of vector dimensionality by changing it from 100 to 50, the minimum count was used between 1 and 2, as well as the number of worker threads that changed from 10 to 5 to 15. The models presented many interesting results with more differences observed between them than similarities, since a few parameters were changed during building each model. "
      ]
    }
  ]
}
